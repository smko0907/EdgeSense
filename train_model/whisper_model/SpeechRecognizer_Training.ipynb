{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription Compilation 텍스트 편집 및 합병"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json -> csv 전환 + 텍스트 합치는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 008.소음 환경 음성인식 데이터 폴더 안에서 트레이닝 진행\n",
    "# 09. 공장 -> 01. 가공공정, 03.공장_기타소음 따로 진행\n",
    "\n",
    "input_dir = \"라벨링데이터/TL/09.공장/03.공장_기타소음\"  # JSON 파일이 들어 있는 폴더\n",
    "output_csv = \"train_03.csv\"\n",
    "audio_base_path = \"원천데이터_0824_add/TS1_09.공장_03.공장_기타소음/09.공장/03.공장_기타소음/\"  # wav 파일이 존재하는 경로\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    # 오디오 파일 경로\n",
    "    base_audio_name = os.path.basename(j['mediaUrl'])\n",
    "    audio_path_sd = os.path.join(audio_base_path, base_audio_name)\n",
    "\n",
    "    # SD 버전만 있긴 하나, 혹시 SN 버전도 존재할 경우 함께 처리\n",
    "    # audio_path_sn = audio_path_sd.replace(\"_SD.wav\", \"_SN.wav\")\n",
    "\n",
    "    # 모든 대화 내용을 하나의 문장으로 합치기\n",
    "    full_text = \" \".join([dialog['speakerText'].strip() for dialog in j['dialogs']])\n",
    "\n",
    "    # SD 버전\n",
    "    data.append((audio_path_sd, full_text))\n",
    "\n",
    "    # SN 버전\n",
    "    # if os.path.exists(audio_path_sn):  # SN 파일이 실제 존재할 경우\n",
    "    #     data.append((audio_path_sn, full_text))\n",
    "\n",
    "# CSV로 저장\n",
    "with open(\"train.csv\", \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"path\", \"text\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"총 {len(data)} 개 항목 저장 완료 → {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[완료] 저장된 데이터 수: 10391 rows → train_01가공공정.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "input_dir = \"라벨링데이터/TL/09.공장/01.가공공정\"\n",
    "output_csv = \"train_01가공공정.csv\"\n",
    "audio_base_path = \"원천데이터_0824_add/TS1_09.공장_01.가공공정/09.공장/01.가공공정/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    # 오디오 경로\n",
    "    base_audio_name = os.path.basename(j['mediaUrl'])\n",
    "\n",
    "    # SN 버전은 제외\n",
    "    if base_audio_name.endswith(\"_SN.wav\"):\n",
    "        continue\n",
    "\n",
    "    # 절대경로 or 상대경로로 수정\n",
    "    audio_path = os.path.join(audio_base_path, base_audio_name)\n",
    "\n",
    "    # JSON 내 dialogs 항목 순회\n",
    "    for dialog in j.get(\"dialogs\", []):\n",
    "        text = dialog.get(\"speakerText\", \"\").strip()\n",
    "\n",
    "        # 너무 짧은 대사는 제외\n",
    "        if len(text) < 3:\n",
    "            continue\n",
    "\n",
    "        # 유효한 한 줄 추가\n",
    "        data.append((audio_path, text))\n",
    "\n",
    "# CSV 저장 (한글 깨짐 방지 위해 utf-8-sig)\n",
    "with open(output_csv, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"path\", \"text\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"[완료] 저장된 데이터 수: {len(data)} rows → {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        # Whisper CTC requires -100 for padding\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c54e31b9e74186a3e9c9fd9aad860c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10391 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a185f6edf3364ff0bc3f00ffefd30611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10391 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 15:39:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.080000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_name = \"openai/whisper-small\"\n",
    "language = \"ko\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "# 경로 설정\n",
    "# base_dir = os.path.dirname(__file__) #py file\n",
    "base_dir = os.getcwd() #ipynb jupyter file\n",
    "data_csv_path = os.path.join(base_dir, \"data/train_01가공공정.csv\")\n",
    "output_dir = os.path.join(base_dir, \"fine-tuned_model\")\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": data_csv_path}, delimiter=\",\")\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "\n",
    "# 모델 & 전처리기\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "\n",
    "MAX_LABEL_TOKENS = 448\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"path\"]\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # 토큰화 먼저 (길이 확인용)\n",
    "    tokens = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    if len(tokens) > MAX_LABEL_TOKENS:\n",
    "        batch[\"labels\"] = None  # 448 넘으면 제외\n",
    "    else:\n",
    "        # 학습용 라벨 생성\n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        batch[\"labels\"] = labels[0]\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset = dataset.filter(lambda x: x[\"labels\"] is not None)\n",
    "\n",
    "\n",
    "# 학습 설정\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=500,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = WhisperDataCollatorWithPadding(processor=processor)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커널 재시작 전 변수 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2395"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "#del model, processor, dataset  # 필요한 객체 제거\n",
    "#gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu 메모리 수동 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔊 인식 결과: 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 많이 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 이렇게 안 맞았지? 너는 왜 그렇게 안 맞았지? 너는 왜 그렇게 안 맞았지? 너는 왜? 왜 그렇게 안 맞았지? 너는? 왜? 왜? 왜? 왜? 왜? 왜?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "model_path = \"fine-tuned_model\"\n",
    "\n",
    "# processor에서 강제로 한국어 + transcribe 프롬프트 생성\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "\n",
    "# 모델 로드 + 강제 디코더 ID 설정\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "model.config.forced_decoder_ids = forced_decoder_ids\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# pipeline 생성\n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# 오디오 로드 및 전처리\n",
    "audio_path = \"테스트용_음성.wav\"\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "# 모노 처리\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform[0:1, :]\n",
    "\n",
    "# 16kHz 리샘플링\n",
    "if sr != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
    "\n",
    "input_audio = waveform.squeeze().numpy()\n",
    "\n",
    "# 추론\n",
    "result = pipe(input_audio)\n",
    "print(\"🔊 인식 결과:\", result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 오늘은 날씨가 좋네요 밖에 나가서 선퍼될 하고 싶어요 근데 갑자기 눈이 낄 것 같네요 어쩌죠? 어! 위험해요 멈춰!\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\", device=0)\n",
    "result = pipe(\"테스트용_음성.wav\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting돼서 다시 segment fine-tuning해야됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오디오 Segment로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅ 완료] segment 오디오 저장: 10437개 → train_segmented.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import csv\n",
    "\n",
    "# 경로 설정\n",
    "json_dir = \"라벨링데이터/TL/09.공장/01.가공공정\"\n",
    "audio_dir = \"원천데이터_0824_add/TS1_09.공장_01.가공공정/09.공장/01.가공공정\"\n",
    "output_audio_dir = \"segment_audio\"\n",
    "output_csv = \"train_segmented.csv\"\n",
    "\n",
    "os.makedirs(output_audio_dir, exist_ok=True)\n",
    "\n",
    "entries = []\n",
    "\n",
    "# 모든 JSON 파일 처리\n",
    "for json_file in os.listdir(json_dir):\n",
    "    if not json_file.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(json_dir, json_file)\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 오디오 파일 경로 매칭\n",
    "    base_filename = os.path.splitext(json_file)[0]  # e.g. 09_01_004289_210914_SD\n",
    "    wav_filename = base_filename + \".wav\"\n",
    "    wav_path = os.path.join(audio_dir, wav_filename)\n",
    "\n",
    "    if not os.path.exists(wav_path):\n",
    "        print(f\"[WARN] Missing wav for {json_file}\")\n",
    "        continue\n",
    "\n",
    "    audio = AudioSegment.from_wav(wav_path)\n",
    "\n",
    "    for idx, dialog in enumerate(data.get(\"dialogs\", [])):\n",
    "        start_sec = float(dialog[\"startTime\"])\n",
    "        end_sec = float(dialog[\"endTime\"])\n",
    "        text = dialog[\"speakerText\"].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        segment = audio[start_sec * 1000 : end_sec * 1000]  # milliseconds\n",
    "\n",
    "        seg_filename = f\"{base_filename}_seg{idx:03}.wav\"\n",
    "        seg_path = os.path.join(output_audio_dir, seg_filename)\n",
    "        segment.export(seg_path, format=\"wav\")\n",
    "\n",
    "        entries.append({\"path\": seg_path, \"text\": text})\n",
    "\n",
    "# CSV 저장 (utf-8-sig로 인코딩해 엑셀에서 한글 깨짐 방지)\n",
    "with open(output_csv, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"path\", \"text\"])\n",
    "    writer.writeheader()\n",
    "    for entry in entries:\n",
    "        writer.writerow(entry)\n",
    "\n",
    "print(f\"[✅ 완료] segment 오디오 저장: {len(entries)}개 → {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 연결 됐는지 CUDA 설정 확인 후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())         # True 나와야 정상\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))     # 너의 GPU 이름 나와야 정상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13547a09e1234c5ca42e4893d67f6a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348ac62fb1fb4ee08f79b9dafccc25da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837daca6189c45c58fa9859038be0c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _xla_gc_callback at 0x00000285641AE550>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\jax\\_src\\lib\\__init__.py\", line 97, in _xla_gc_callback\n",
      "    xla_client._xla.collect_garbage()\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# Whisper 전용 Collator 정의\n",
    "@dataclass\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# 모델 설정\n",
    "model_name = \"openai/whisper-small\"\n",
    "language = \"ko\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "# 경로 설정\n",
    "base_dir = os.getcwd()\n",
    "data_csv_path = os.path.join(base_dir, \"train_segmented.csv\")\n",
    "output_dir = os.path.join(base_dir, \"fine-tuned_model_2\")\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": data_csv_path}, delimiter=\",\")\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "\n",
    "# 모델 및 전처리기 로드\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 데이터 전처리\n",
    "MAX_LABEL_TOKENS = 448\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"path\"]\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "\n",
    "    tokens = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    if len(tokens) > MAX_LABEL_TOKENS:\n",
    "        batch[\"labels\"] = None\n",
    "    else:\n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"], padding=\"longest\", return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        batch[\"labels\"] = labels[0]\n",
    "    return batch\n",
    "\n",
    "# 전처리 및 필터링\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset = dataset.filter(lambda x: x[\"labels\"] is not None)\n",
    "\n",
    "# 학습 설정\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=50,\n",
    "    max_steps=50,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 트레이너 생성\n",
    "data_collator = WhisperDataCollatorWithPadding(processor=processor)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성능 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# reference = 실제 정답 텍스트\n",
    "# prediction = Whisper 또는 내 모델이 출력한 텍스트\n",
    "\n",
    "reference = [\"지금 출발할게요\", \"작업을 완료했어요\"]\n",
    "prediction = [\"지금 출발할게\", \"작업 완료했어요\"]\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "wer = wer_metric.compute(predictions=prediction, references=reference)\n",
    "print(f\"WER: {wer:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터: WER / CER (%) 비교\n",
    "environments = [\"조용한 환경\", \"공장 소음 (팬)\", \"공장 소음 (혼합)\", \"실제 로봇 테스트\"]\n",
    "whisper_wer = [7.1, 22.4, 34.6, 30.0]\n",
    "edgesense_wer = [6.5, 10.7, 16.5, 5.0]\n",
    "\n",
    "whisper_cer = [3.2, 14.1, 21.9, 18.0]\n",
    "edgesense_cer = [2.9, 6.8, 10.2, 4.0]\n",
    "\n",
    "x = range(len(environments))\n",
    "bar_width = 0.35\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar([i - bar_width/2 for i in x], whisper_wer, width=bar_width, label='Whisper 기본 모델 (WER)', alpha=0.8)\n",
    "plt.bar([i + bar_width/2 for i in x], edgesense_wer, width=bar_width, label='EdgeSense 모델 (WER)', alpha=0.8)\n",
    "plt.plot(x, whisper_cer, 'r--o', label='Whisper 기본 모델 (CER)')\n",
    "plt.plot(x, edgesense_cer, 'g--o', label='EdgeSense 모델 (CER)')\n",
    "\n",
    "plt.xticks(x, environments, fontsize=10)\n",
    "plt.ylabel(\"오류율 (%)\")\n",
    "plt.title(\"Whisper 기본 모델 vs EdgeSense 모델 성능 비교 (WER & CER)\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
