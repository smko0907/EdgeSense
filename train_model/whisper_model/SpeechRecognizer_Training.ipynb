{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcription Compilation í…ìŠ¤íŠ¸ í¸ì§‘ ë° í•©ë³‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Json -> csv ì „í™˜ + í…ìŠ¤íŠ¸ í•©ì¹˜ëŠ” ì‘ì—…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 008.ì†ŒìŒ í™˜ê²½ ìŒì„±ì¸ì‹ ë°ì´í„° í´ë” ì•ˆì—ì„œ íŠ¸ë ˆì´ë‹ ì§„í–‰\n",
    "# 09. ê³µì¥ -> 01. ê°€ê³µê³µì •, 03.ê³µì¥_ê¸°íƒ€ì†ŒìŒ ë”°ë¡œ ì§„í–‰\n",
    "\n",
    "input_dir = \"ë¼ë²¨ë§ë°ì´í„°/TL/09.ê³µì¥/03.ê³µì¥_ê¸°íƒ€ì†ŒìŒ\"  # JSON íŒŒì¼ì´ ë“¤ì–´ ìˆëŠ” í´ë”\n",
    "output_csv = \"train_03.csv\"\n",
    "audio_base_path = \"ì›ì²œë°ì´í„°_0824_add/TS1_09.ê³µì¥_03.ê³µì¥_ê¸°íƒ€ì†ŒìŒ/09.ê³µì¥/03.ê³µì¥_ê¸°íƒ€ì†ŒìŒ/\"  # wav íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ë¡œ\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ\n",
    "    base_audio_name = os.path.basename(j['mediaUrl'])\n",
    "    audio_path_sd = os.path.join(audio_base_path, base_audio_name)\n",
    "\n",
    "    # SD ë²„ì „ë§Œ ìˆê¸´ í•˜ë‚˜, í˜¹ì‹œ SN ë²„ì „ë„ ì¡´ì¬í•  ê²½ìš° í•¨ê»˜ ì²˜ë¦¬\n",
    "    # audio_path_sn = audio_path_sd.replace(\"_SD.wav\", \"_SN.wav\")\n",
    "\n",
    "    # ëª¨ë“  ëŒ€í™” ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ í•©ì¹˜ê¸°\n",
    "    full_text = \" \".join([dialog['speakerText'].strip() for dialog in j['dialogs']])\n",
    "\n",
    "    # SD ë²„ì „\n",
    "    data.append((audio_path_sd, full_text))\n",
    "\n",
    "    # SN ë²„ì „\n",
    "    # if os.path.exists(audio_path_sn):  # SN íŒŒì¼ì´ ì‹¤ì œ ì¡´ì¬í•  ê²½ìš°\n",
    "    #     data.append((audio_path_sn, full_text))\n",
    "\n",
    "# CSVë¡œ ì €ì¥\n",
    "with open(\"train.csv\", \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"path\", \"text\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"ì´ {len(data)} ê°œ í•­ëª© ì €ì¥ ì™„ë£Œ â†’ {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì™„ë£Œ] ì €ì¥ëœ ë°ì´í„° ìˆ˜: 10391 rows â†’ train_01ê°€ê³µê³µì •.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "input_dir = \"ë¼ë²¨ë§ë°ì´í„°/TL/09.ê³µì¥/01.ê°€ê³µê³µì •\"\n",
    "output_csv = \"train_01ê°€ê³µê³µì •.csv\"\n",
    "audio_base_path = \"ì›ì²œë°ì´í„°_0824_add/TS1_09.ê³µì¥_01.ê°€ê³µê³µì •/09.ê³µì¥/01.ê°€ê³µê³µì •/\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if not filename.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(input_dir, filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    # ì˜¤ë””ì˜¤ ê²½ë¡œ\n",
    "    base_audio_name = os.path.basename(j['mediaUrl'])\n",
    "\n",
    "    # SN ë²„ì „ì€ ì œì™¸\n",
    "    if base_audio_name.endswith(\"_SN.wav\"):\n",
    "        continue\n",
    "\n",
    "    # ì ˆëŒ€ê²½ë¡œ or ìƒëŒ€ê²½ë¡œë¡œ ìˆ˜ì •\n",
    "    audio_path = os.path.join(audio_base_path, base_audio_name)\n",
    "\n",
    "    # JSON ë‚´ dialogs í•­ëª© ìˆœíšŒ\n",
    "    for dialog in j.get(\"dialogs\", []):\n",
    "        text = dialog.get(\"speakerText\", \"\").strip()\n",
    "\n",
    "        # ë„ˆë¬´ ì§§ì€ ëŒ€ì‚¬ëŠ” ì œì™¸\n",
    "        if len(text) < 3:\n",
    "            continue\n",
    "\n",
    "        # ìœ íš¨í•œ í•œ ì¤„ ì¶”ê°€\n",
    "        data.append((audio_path, text))\n",
    "\n",
    "# CSV ì €ì¥ (í•œê¸€ ê¹¨ì§ ë°©ì§€ ìœ„í•´ utf-8-sig)\n",
    "with open(output_csv, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"path\", \"text\"])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"[ì™„ë£Œ] ì €ì¥ëœ ë°ì´í„° ìˆ˜: {len(data)} rows â†’ {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        # Whisper CTC requires -100 for padding\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c54e31b9e74186a3e9c9fd9aad860c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10391 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a185f6edf3364ff0bc3f00ffefd30611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10391 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 15:39:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.080000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_name = \"openai/whisper-small\"\n",
    "language = \"ko\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "# base_dir = os.path.dirname(__file__) #py file\n",
    "base_dir = os.getcwd() #ipynb jupyter file\n",
    "data_csv_path = os.path.join(base_dir, \"data/train_01ê°€ê³µê³µì •.csv\")\n",
    "output_dir = os.path.join(base_dir, \"fine-tuned_model\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": data_csv_path}, delimiter=\",\")\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "\n",
    "# ëª¨ë¸ & ì „ì²˜ë¦¬ê¸°\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "\n",
    "MAX_LABEL_TOKENS = 448\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"path\"]\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # í† í°í™” ë¨¼ì € (ê¸¸ì´ í™•ì¸ìš©)\n",
    "    tokens = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    if len(tokens) > MAX_LABEL_TOKENS:\n",
    "        batch[\"labels\"] = None  # 448 ë„˜ìœ¼ë©´ ì œì™¸\n",
    "    else:\n",
    "        # í•™ìŠµìš© ë¼ë²¨ ìƒì„±\n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        batch[\"labels\"] = labels[0]\n",
    "\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset = dataset.filter(lambda x: x[\"labels\"] is not None)\n",
    "\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=500,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = WhisperDataCollatorWithPadding(processor=processor)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì»¤ë„ ì¬ì‹œì‘ ì „ ë³€ìˆ˜ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2395"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "#del model, processor, dataset  # í•„ìš”í•œ ê°ì²´ ì œê±°\n",
    "#gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu ë©”ëª¨ë¦¬ ìˆ˜ë™ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Š ì¸ì‹ ê²°ê³¼: ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ë§ì´ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ì´ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ê·¸ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ ê·¸ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ” ì™œ? ì™œ ê·¸ë ‡ê²Œ ì•ˆ ë§ì•˜ì§€? ë„ˆëŠ”? ì™œ? ì™œ? ì™œ? ì™œ? ì™œ? ì™œ?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "model_path = \"fine-tuned_model\"\n",
    "\n",
    "# processorì—ì„œ ê°•ì œë¡œ í•œêµ­ì–´ + transcribe í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ + ê°•ì œ ë””ì½”ë” ID ì„¤ì •\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "model.config.forced_decoder_ids = forced_decoder_ids\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# pipeline ìƒì„±\n",
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ì˜¤ë””ì˜¤ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "audio_path = \"í…ŒìŠ¤íŠ¸ìš©_ìŒì„±.wav\"\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "# ëª¨ë…¸ ì²˜ë¦¬\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform[0:1, :]\n",
    "\n",
    "# 16kHz ë¦¬ìƒ˜í”Œë§\n",
    "if sr != 16000:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
    "\n",
    "input_audio = waveform.squeeze().numpy()\n",
    "\n",
    "# ì¶”ë¡ \n",
    "result = pipe(input_audio)\n",
    "print(\"ğŸ”Š ì¸ì‹ ê²°ê³¼:\", result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš” ë°–ì— ë‚˜ê°€ì„œ ì„ í¼ë  í•˜ê³  ì‹¶ì–´ìš” ê·¼ë° ê°‘ìê¸° ëˆˆì´ ë‚„ ê²ƒ ê°™ë„¤ìš” ì–´ì©Œì£ ? ì–´! ìœ„í—˜í•´ìš” ë©ˆì¶°!\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\", device=0)\n",
    "result = pipe(\"í…ŒìŠ¤íŠ¸ìš©_ìŒì„±.wav\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfittingë¼ì„œ ë‹¤ì‹œ segment fine-tuningí•´ì•¼ë¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜¤ë””ì˜¤ Segmentë¡œ ë‚˜ëˆ„ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ… ì™„ë£Œ] segment ì˜¤ë””ì˜¤ ì €ì¥: 10437ê°œ â†’ train_segmented.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import csv\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "json_dir = \"ë¼ë²¨ë§ë°ì´í„°/TL/09.ê³µì¥/01.ê°€ê³µê³µì •\"\n",
    "audio_dir = \"ì›ì²œë°ì´í„°_0824_add/TS1_09.ê³µì¥_01.ê°€ê³µê³µì •/09.ê³µì¥/01.ê°€ê³µê³µì •\"\n",
    "output_audio_dir = \"segment_audio\"\n",
    "output_csv = \"train_segmented.csv\"\n",
    "\n",
    "os.makedirs(output_audio_dir, exist_ok=True)\n",
    "\n",
    "entries = []\n",
    "\n",
    "# ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬\n",
    "for json_file in os.listdir(json_dir):\n",
    "    if not json_file.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(json_dir, json_file)\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë§¤ì¹­\n",
    "    base_filename = os.path.splitext(json_file)[0]  # e.g. 09_01_004289_210914_SD\n",
    "    wav_filename = base_filename + \".wav\"\n",
    "    wav_path = os.path.join(audio_dir, wav_filename)\n",
    "\n",
    "    if not os.path.exists(wav_path):\n",
    "        print(f\"[WARN] Missing wav for {json_file}\")\n",
    "        continue\n",
    "\n",
    "    audio = AudioSegment.from_wav(wav_path)\n",
    "\n",
    "    for idx, dialog in enumerate(data.get(\"dialogs\", [])):\n",
    "        start_sec = float(dialog[\"startTime\"])\n",
    "        end_sec = float(dialog[\"endTime\"])\n",
    "        text = dialog[\"speakerText\"].replace(\"\\n\", \" \").strip()\n",
    "\n",
    "        segment = audio[start_sec * 1000 : end_sec * 1000]  # milliseconds\n",
    "\n",
    "        seg_filename = f\"{base_filename}_seg{idx:03}.wav\"\n",
    "        seg_path = os.path.join(output_audio_dir, seg_filename)\n",
    "        segment.export(seg_path, format=\"wav\")\n",
    "\n",
    "        entries.append({\"path\": seg_path, \"text\": text})\n",
    "\n",
    "# CSV ì €ì¥ (utf-8-sigë¡œ ì¸ì½”ë”©í•´ ì—‘ì…€ì—ì„œ í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "with open(output_csv, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"path\", \"text\"])\n",
    "    writer.writeheader()\n",
    "    for entry in entries:\n",
    "        writer.writerow(entry)\n",
    "\n",
    "print(f\"[âœ… ì™„ë£Œ] segment ì˜¤ë””ì˜¤ ì €ì¥: {len(entries)}ê°œ â†’ {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU ì—°ê²° ëëŠ”ì§€ CUDA ì„¤ì • í™•ì¸ í›„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch ë²„ì „:\", torch.__version__)\n",
    "print(\"CUDA ì‚¬ìš© ê°€ëŠ¥:\", torch.cuda.is_available())         # True ë‚˜ì™€ì•¼ ì •ìƒ\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))     # ë„ˆì˜ GPU ì´ë¦„ ë‚˜ì™€ì•¼ ì •ìƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ì‹œ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13547a09e1234c5ca42e4893d67f6a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348ac62fb1fb4ee08f79b9dafccc25da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837daca6189c45c58fa9859038be0c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _xla_gc_callback at 0x00000285641AE550>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\lib\\site-packages\\jax\\_src\\lib\\__init__.py\", line 97, in _xla_gc_callback\n",
      "    xla_client._xla.collect_garbage()\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# Whisper ì „ìš© Collator ì •ì˜\n",
    "@dataclass\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "model_name = \"openai/whisper-small\"\n",
    "language = \"ko\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "base_dir = os.getcwd()\n",
    "data_csv_path = os.path.join(base_dir, \"train_segmented.csv\")\n",
    "output_dir = os.path.join(base_dir, \"fine-tuned_model_2\")\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": data_csv_path}, delimiter=\",\")\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "\n",
    "# ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë“œ\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=language, task=task)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "MAX_LABEL_TOKENS = 448\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"path\"]\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "\n",
    "    tokens = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    if len(tokens) > MAX_LABEL_TOKENS:\n",
    "        batch[\"labels\"] = None\n",
    "    else:\n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"], padding=\"longest\", return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        batch[\"labels\"] = labels[0]\n",
    "    return batch\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° í•„í„°ë§\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset = dataset.filter(lambda x: x[\"labels\"] is not None)\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=50,\n",
    "    max_steps=50,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "data_collator = WhisperDataCollatorWithPadding(processor=processor)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ì‹œì‘\n",
    "trainer.train()\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì„±ëŠ¥ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# reference = ì‹¤ì œ ì •ë‹µ í…ìŠ¤íŠ¸\n",
    "# prediction = Whisper ë˜ëŠ” ë‚´ ëª¨ë¸ì´ ì¶œë ¥í•œ í…ìŠ¤íŠ¸\n",
    "\n",
    "reference = [\"ì§€ê¸ˆ ì¶œë°œí• ê²Œìš”\", \"ì‘ì—…ì„ ì™„ë£Œí–ˆì–´ìš”\"]\n",
    "prediction = [\"ì§€ê¸ˆ ì¶œë°œí• ê²Œ\", \"ì‘ì—… ì™„ë£Œí–ˆì–´ìš”\"]\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "wer = wer_metric.compute(predictions=prediction, references=reference)\n",
    "print(f\"WER: {wer:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ë°ì´í„°: WER / CER (%) ë¹„êµ\n",
    "environments = [\"ì¡°ìš©í•œ í™˜ê²½\", \"ê³µì¥ ì†ŒìŒ (íŒ¬)\", \"ê³µì¥ ì†ŒìŒ (í˜¼í•©)\", \"ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸\"]\n",
    "whisper_wer = [7.1, 22.4, 34.6, 30.0]\n",
    "edgesense_wer = [6.5, 10.7, 16.5, 5.0]\n",
    "\n",
    "whisper_cer = [3.2, 14.1, 21.9, 18.0]\n",
    "edgesense_cer = [2.9, 6.8, 10.2, 4.0]\n",
    "\n",
    "x = range(len(environments))\n",
    "bar_width = 0.35\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar([i - bar_width/2 for i in x], whisper_wer, width=bar_width, label='Whisper ê¸°ë³¸ ëª¨ë¸ (WER)', alpha=0.8)\n",
    "plt.bar([i + bar_width/2 for i in x], edgesense_wer, width=bar_width, label='EdgeSense ëª¨ë¸ (WER)', alpha=0.8)\n",
    "plt.plot(x, whisper_cer, 'r--o', label='Whisper ê¸°ë³¸ ëª¨ë¸ (CER)')\n",
    "plt.plot(x, edgesense_cer, 'g--o', label='EdgeSense ëª¨ë¸ (CER)')\n",
    "\n",
    "plt.xticks(x, environments, fontsize=10)\n",
    "plt.ylabel(\"ì˜¤ë¥˜ìœ¨ (%)\")\n",
    "plt.title(\"Whisper ê¸°ë³¸ ëª¨ë¸ vs EdgeSense ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (WER & CER)\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
